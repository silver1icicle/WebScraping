{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Website URL: https://www.chicagoreader.com/\n",
    "## Tutorial Reference: http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/\n",
    "#### NOTE: Instant protocol help: shift + tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from bs4 import BeautifulSoup\n",
    "try:\n",
    "    from urllib.request import urlopen      # For Python 3.x compatibility...In 3.x, the urllib2 module has been split into urllib.request and urllib.error \n",
    "except:\n",
    "    from urllib2 import urlopen             # For Python 2.x compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mimic C's MACROS\n",
    "BASE_URL = \"https://www.chicagoreader.com/\"                                                               # Launch Page\n",
    "PAGE_URL = \"https://www.chicagoreader.com/chicago/best-of-chicago-2011-food-drink/BestOf?oid=4106228\"     # Test Purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_soup(section_url):\n",
    "    '''\n",
    "        Obj: To eliminate the redundant tasks being done in the remaining core scraping functions. Its an intrinsic function\n",
    "        Input: The URL of the page (Specific section/Specific category within the section) we want to scrape  \n",
    "        Output: an instance of the BeautifulSoup class\n",
    "        Benifit: If in the future you decide to change the parser, you need to make changes only @ one place. \n",
    "    '''\n",
    "    html = urlopen(section_url).read()       # Opens the specified URL [You get to see the \"View Page Source\" Content]\n",
    "    #print(html)\n",
    "    soup = BeautifulSoup(html, \"lxml\")       # Arg1: Actual Markup    Arg2: The Parser [html.parser/lxml/html5lib].The lxml parser has two versions, an HTML parser and an XML parser.\n",
    "                                             # The lxml parser is very fast and can be used to quickly parse given HTML\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_category_links(section_url):\n",
    "    '''\n",
    "        Obj  : To return the hyperlinks of individual categories. Its an intrinsic function\n",
    "        Input: section_url: URL of the specific section you are targeting. In this case you are focusing on \"Food & Drink\"\n",
    "        Output: A list of all category links, present on the web page of \"Food & Drink\". \n",
    "    '''\n",
    "    soup = create_soup(section_url) \n",
    "    #print(soup)    \n",
    "    boccat = soup.find(\"dl\", \"boccat\")       # Return only the first child of this Tag  \n",
    "    #print(type(boccat))\n",
    "    category_links = [BASE_URL + dd.a['href'] for dd in boccat.findAll(\"dd\")]    # findAll: Extracts a list of Tag objects that match the given criteria\n",
    "    #print(category_links)\n",
    "    return category_links\n",
    "\n",
    "#Test:\n",
    "#get_category_links(PAGE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_catgeory_winner_runnerup(specific_category_url):\n",
    "    '''\n",
    "        Obj: Fetch the winner and the runner-up details. Its an intrinsic function\n",
    "        Input: The specific category URL\n",
    "        Output: A dictionary comprising of Category/Category URL/Winner's Name/Runner Ups\n",
    "    '''\n",
    "    soup = create_soup(specific_category_url) \n",
    "    category = soup.find(\"h1\", \"headline\").string              # Get the name of the category in consideration\n",
    "    #print(category)\n",
    "    winners = [i.string for i in soup.findAll(\"h2\", \"boc1\")]   # To get the string part of the anchor tag\n",
    "    runnerups = [i.string for i in soup.findAll(\"h2\",\"boc2\")]  # To get the string part of the anchor tag\n",
    "    #print(winners)\n",
    "    #print(runnerups)\n",
    "    return {\"category\":category,\n",
    "            \"category_url\":specific_category_url,\n",
    "            \"winners\":winners, \n",
    "            \"runnerups\":runnerups}\n",
    "\n",
    "#Test:    \n",
    "#get_catgeory_winner_runnerup(\"https://www.chicagoreader.com//chicago/BestOf?category=1979894&year=2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def persist_scraped_data():\n",
    "    '''\n",
    "        Obj   : To persist the scraped data in a CSV file\n",
    "        Input : None\n",
    "        Output: None [Writes the scraped results to a CSV file]\n",
    "    '''\n",
    "    import csv\n",
    "    allCategoryLinksList = get_category_links(PAGE_URL)       # Fetch all categories within the \"Food & Drink\" section\n",
    "    with open(\"BestChicagoFoods_N_Drinks.csv\", \"w\", newline='\\n') as f:\n",
    "        csvWriter = csv.writer(f, delimiter=',')\n",
    "        csvWriter.writerow([\"Category\", \"Category URL\", \"Winners\", \"Runner Ups\"])\n",
    "        tempStr = \"\"\n",
    "        for catLink in allCategoryLinksList:\n",
    "            tempRes = get_catgeory_winner_runnerup(catLink)\n",
    "            #print(tempRes)\n",
    "            #Fetch the individual key:values [As dicts wont maintain the specified order, alternatively use OrderedDict]\n",
    "            tempCat = tempRes['category'] if tempRes['category'] else '-NA-'\n",
    "            tempCatURL = tempRes['category_url'] if tempRes['category_url'] else \"-NA-\"\n",
    "            tempWin = '|'.join(tempRes['winners']) if tempRes['winners'] else \"-NA-\"\n",
    "            tempRun = '|'.join(tempRes['runnerups']) if tempRes['runnerups'] else \"-NA-\"    \n",
    "            csvWriter.writerow([tempCat, tempCatURL, tempWin, tempRun])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping results persisted!!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    persist_scraped_data()\n",
    "    print(\"Web scraping results persisted!!\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
